{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Assignment\n"
      ],
      "metadata": {
        "id": "RgZTKE7NsPat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "##### #Ans.A Decision Tree is a supervised machine learning algorithm used for both classification and regression. In classification tasks, it predicts the class label of an instance by learning simple if-else rules from the data.\n",
        "\n",
        "How it Works:\n",
        "\n",
        "1. Root Node:\\\n",
        "The process starts with the entire dataset at the root. The algorithm chooses the feature that best separates the classes using criteria such as Gini Index or Information Gain.\n",
        "\n",
        "2. Splitting:\\\n",
        "The data is divided into subsets based on feature values. Each decision splits the dataset into smaller, more homogeneous groups.\n",
        "\n",
        "3. Decision Nodes & Branches:\\\n",
        "Internal nodes represent decisions based on features, and branches represent the outcomes of those decisions.\n",
        "\n",
        "4. Leaf Nodes:\\\n",
        "Splitting continues until a stopping condition is reached (e.g., all samples belong to one class, or no further improvement is possible). Each leaf node represents a final class label.\n",
        "\n",
        "Example (Binary Classification):\n",
        "\n",
        "If we want to classify whether a person buys a laptop (Yes/No) based on Age and Income:\n",
        "* Root Node: Is Age > 30?\n",
        "   * If No → Predict \"No\"\n",
        "   * If Yes → Move to next decision\n",
        "* Decision Node: Is Income > 50,000?\n",
        "   * If Yes → Predict \"Yes\"\n",
        "   * If No → Predict \"No\""
      ],
      "metadata": {
        "id": "q-31xl_8-CNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "##### #Ans.In Decision Trees, impurity measures help determine how well a feature splits the data. The goal is to create nodes that are as pure as possible, meaning most or all samples in a node belong to the same class. Two common measures are Gini Impurity and Entropy.\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "* Measures the probability of misclassifying a randomly chosen sample if it is labeled according to the class distribution in the node.\n",
        "* Formula:\\\n",
        "Gini = 1 - Σ (p_i²)\n",
        "where p_i is the proportion of class i in the node.\n",
        "* A Gini value of 0 means the node is pure (all samples belong to one class).\n",
        "\n",
        "2. Entropy\n",
        "* Measures the uncertainty or disorder in the node.\n",
        "* Formula:\\\n",
        "Entropy = - Σ (p_i * log2(p_i))\n",
        "* Entropy = 0 indicates a pure node, and higher values indicate more mixed classes.\n",
        "* Information Gain is calculated as the reduction in entropy after a split.\n",
        "\n",
        "Impact on Splits\n",
        "* Both measures guide the tree to select splits that create purer subsets.\n",
        "* Gini is simpler and faster to compute.\n",
        "* Entropy is more sensitive to class distribution and can sometimes produce slightly different splits.\n",
        "* The chosen split maximizes the reduction in impurity (or maximizes Information Gain), improving classification accuracy."
      ],
      "metadata": {
        "id": "zwa4pcEiBOAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "##### #Ans.Pre-Pruning (Early Stopping)\n",
        "* The tree's growth is stopped early based on certain conditions before it becomes overly complex.\n",
        "\n",
        "* Common stopping criteria:\n",
        "\n",
        "   * Maximum depth (max_depth)\n",
        "   * Minimum samples to split (min_samples_split)\n",
        "   * Minimum samples in a leaf (min_samples_leaf)\n",
        "   * Minimum impurity decrease\n",
        "* Practical advantage: Saves computation time and prevents overfitting by keeping the model simpler from the start.\n",
        "\n",
        "Post-Pruning (Prune After Full Growth)\n",
        "* The tree is grown to its maximum size (or nearly so), then branches that provide little predictive power are removed.\n",
        "* Techniques:\n",
        "   * Reduced Error Pruning (evaluate on validation set and remove unhelpful branches)\n",
        "   * Cost Complexity Pruning (ccp_alpha in scikit-learn)\n",
        "* Practical advantage: Allows the model to initially capture complex patterns and then simplifies it for better generalization, often leading to improved accuracy on unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "3lBK6_ATfOP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "##### #Ans.Information Gain (IG) is a measure used in Decision Trees to quantify how much a feature improves the purity of a node. It calculates the reduction in entropy (uncertainty or disorder) after splitting the dataset based on a feature.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Information Gain = Entropy(Parent) - Σ ( (size of child / size of parent) x Entropy(Child) )\n",
        "\n",
        "\n",
        "* Entropy(Parent): Entropy of the dataset before the split.\n",
        "* Entropy(Child): Entropy of each subset created after the split.\n",
        "\n",
        "Importance in Choosing Splits:\n",
        "\n",
        "* Information Gain helps the tree decide which feature to split on at each node.\n",
        "* The feature that produces the highest Information Gain (i.e., maximally reduces uncertainty) is chosen.\n",
        "* This ensures that the resulting subsets are more homogeneous, leading to a tree that classifies data more accurately.\n",
        "\n",
        "Example:\n",
        "If splitting on Feature A reduces entropy from 0.9 to 0.4, while Feature B reduces it to 0.6, the tree will choose Feature A for the split because it gives higher Information Gain.\n",
        "\n"
      ],
      "metadata": {
        "id": "aNFdfHsRgjvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### #Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "##### #Ans.Common Real-World Applications:\n",
        "\n",
        "1. Medical Diagnosis: Helps predict whether a patient has a particular disease using symptoms and test results.\n",
        "2. Credit Scoring: Classifies loan applicants as “low risk” or “high risk” based on their financial background.\n",
        "3. Fraud Detection: Identifies unusual or suspicious transactions in financial systems.\n",
        "4. Customer Churn Prediction: Predicts which customers are likely to stop using a service.\n",
        "5. Product Recommendation: Suggests products to users based on their past behavior and preferences.\n",
        "6. Regression Problems: Can be used to estimate continuous values such as house prices or sales forecasts.\n",
        "\n",
        "Main Advantages:\n",
        "* Interpretability: Decision Trees generate easy-to-read, human-understandable rules.\n",
        "* Versatility: Can handle both numerical and categorical data without complex preprocessing.\n",
        "* No Feature Scaling Required: Works without normalization or standardization of data.\n",
        "* Ability to Model Non-Linear Relationships: Can capture complex patterns in the data.\n",
        "* Feature Importance: Highlights which variables have the most influence on predictions.\n",
        "\n",
        "\n",
        "Main Limitations:\n",
        "\n",
        "* Overfitting: Deep or fully grown trees may memorize the training data, reducing performance on new data.\n",
        "* High Variance: Small changes in the training dataset can result in very different trees.\n",
        "* Bias Toward High-Cardinality Features: Features with many unique values may dominate the splits.\n",
        "* Lower Accuracy Compared to Ensemble Methods: Often outperformed by techniques like Random Forests or Gradient Boosting on complex datasets."
      ],
      "metadata": {
        "id": "VZtmMZBSjpBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of Decision Tree: {accuracy:.4f}\\n\")\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"  {name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMJc3TuqldG5",
        "outputId": "6bb3b8de-5e45-4430-e043-66b40795df2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree: 0.9333\n",
            "\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0286\n",
            "  petal length (cm): 0.5412\n",
            "  petal width (cm): 0.4303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Fully-grown tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "# Tree with max_depth=3\n",
        "clf_md3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy of fully-grown tree: {acc_full:.4f}\")\n",
        "print(f\"Accuracy of max_depth=3 tree: {acc_md3:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr0BqMsqmKxm",
        "outputId": "256f9b83-aec4-479c-a221-8529e317bc15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 0.9333\n",
            "Accuracy of max_depth=3 tree: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the California Housing dataset from sklearn\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error (MSE) on test data: {mse:.4f}\\n\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\" - {name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXKkGJIOnLb0",
        "outputId": "92936f72-5a4e-4c26-eedf-db17a5b14985"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) on test data: 0.4952\n",
            "\n",
            "Feature Importances:\n",
            " - MedInc: 0.5285\n",
            " - HouseAge: 0.0519\n",
            " - AveRooms: 0.0530\n",
            " - AveBedrms: 0.0287\n",
            " - Population: 0.0305\n",
            " - AveOccup: 0.1308\n",
            " - Latitude: 0.0937\n",
            " - Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up the grid of parameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearch to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters found\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate the best estimator on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Model accuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifmpNRI9ozSR",
        "outputId": "a102ba5b-d7ca-46db-b45a-0c6d63069f68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "# wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "# mixed data types and some missing values.\n",
        "# Explain the step-by-step process you would follow to:\n",
        "# ● Handle the missing values\n",
        "# ● Encode the categorical features\n",
        "# ● Train a Decision Tree model\n",
        "# ● Tune its hyperparameters\n",
        "# ● Evaluate its performance\n",
        "# And describe what business value this model could provide in the real-world\n",
        "# setting."
      ],
      "metadata": {
        "id": "CgoCEINMpeTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### #Ans.\n",
        "1. Handle Missing Values\n",
        "* Analyze missingness: Examine which data points are missing and whether they are missing at random or follow a pattern. This informs the strategy for handling them.\n",
        "* Imputation methods:\n",
        "   * Numerical features: Fill missing values with mean, median, or use advanced techniques like K-Nearest Neighbors imputation.\n",
        "   * Categorical features: Replace missing values with the most frequent category (mode) or assign a new category such as “Unknown.”\n",
        "* Dropping data: If certain features or rows have too many missing values and cannot be reliably imputed, consider dropping them carefully.\n",
        "* Why it matters: Machine learning models cannot handle missing data directly. Cleaning ensures reliable inputs for the model.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "* Identify categorical variables: Examples include patient gender, blood type, or disease severity.\n",
        "* Encoding methods:\n",
        "   * Nominal categories (no inherent order, e.g., blood type): Use One-Hot Encoding.\n",
        "   * Ordinal categories (ordered, e.g., disease severity): Use Label Encoding or map to numeric scales.\n",
        "* Why it matters: Decision Trees require numeric inputs. Encoding transforms categorical features into a format the model can use.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "* Data splitting: Use an 80-20 or 70-30 split for training and testing, or employ cross-validation to ensure generalization.\n",
        "* Initialize the model: Start with a default Decision Tree classifier.\n",
        "* Train the model: Fit it on the processed training data.\n",
        "* Why Decision Trees: They can handle mixed data types, are interpretable (important in healthcare), and capture non-linear relationships.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "* Important hyperparameters:\n",
        "   * max_depth - controls tree complexity and balances underfitting vs. overfitting.\n",
        "   * min_samples_split & min_samples_leaf - determine the minimum samples required to split a node or form a leaf, affecting generalization.\n",
        "   * max_features - controls the number of features considered for each split.\n",
        "\n",
        "* Tuning method: Use GridSearchCV or RandomizedSearchCV with cross-validation to find the best combination of parameters.\n",
        "* Why it matters: Proper tuning improves predictive performance and reduces the risk of overfitting or underfitting.\n",
        "\n",
        "5. Evaluate Performance\n",
        "\n",
        "* Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
        "* Healthcare priority: Recall (sensitivity) is often most important to ensure patients with the disease are detected, even if false positives occur.\n",
        "* Validation: Test on a separate set or use cross-validation for reliable performance.\n",
        "* Interpretability: Feature importance and tree visualization help explain model decisions to clinicians and stakeholders.\n",
        "\n",
        "Business Value in a Real-World Setting\n",
        "\n",
        "* Early detection: Enables timely treatment, improving patient outcomes and reducing costs.\n",
        "* Resource optimization: Helps prioritize high-risk patients for interventions, making healthcare delivery more efficient.\n",
        "* Personalized care: Supports tailored monitoring and treatment plans based on individual risk profiles.\n",
        "* Data-driven insights: Informs product development, policies, and patient outreach strategies.\n",
        "* Trust and transparency: Decision Trees interpretability builds confidence among clinicians, regulators, and patients, crucial in healthcare environments."
      ],
      "metadata": {
        "id": "8XrGfFtkqLAv"
      }
    }
  ]
}